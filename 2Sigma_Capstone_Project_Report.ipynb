{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Sigma News Challenge - Kaggle competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Two Sigma competition at Kaggle aims at advancing our understanding of how the content of news\n",
    "analytics might influence the performance of stock prices. For this purpose a large set of daily market\n",
    "and news data is provided for a subset of US-listed financial instruments. This data shall be used to\n",
    "train any kind of learning algorithm deemed useful in order to predict future stock market returns.\n",
    "The competition comprises two stages with two different evaluation periods. In the first stage the\n",
    "predictions are tested against historical data of the period 1/1/2017 to 7/31/2018. This stage will be\n",
    "terminated early next year at which time the final submissions of the participating teams must be\n",
    "handed in. The latter will then be evaluated against future data for about six months to identify the\n",
    "best performing submission which will be disclosed 7/15/2019.\n",
    "\n",
    "In some sense te task was to create a very simplified version of a trading robot which imposed some \n",
    "requirements on both data science and coding skills in order to make it all work fast. Kernel-only \n",
    "environment set up was one of the most difficult part of the competition since they were rather unstable \n",
    "and inconvenient.\n",
    "\n",
    "The objective function for this machine learning task is constructed as follows: for each day t within \n",
    "the evaluation period the value xt is calculated as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (<ipython-input-28-07f5335cc323>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-28-07f5335cc323>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    \\begin{equation*}\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "\\begin{equation*}\n",
    "\\x_t & = \\sigma(y-x) \n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (<ipython-input-25-949b531aee94>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-25-949b531aee94>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    \\begin{align}\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "\\begin{align}\n",
    "\\x_t  =\\sum_{i=1}y_{ti}r_{ti}u_{ti}\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where for any financial asset i ∈ {1, ..., m} the term:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%latex\n",
    "\n",
    "\\[y_{ti} \\in \\lbrack \\−1, 1\\rbrack\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stands for the predicted confidence value that it’s ten-day market-adjusted leading return rti ∈ R is either positive or negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " uti ∈ {0, 1} \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is the universe variable which controls whether the asset i is included in the evaluation at the particular evaluation day t. \n",
    "\n",
    "Finally, the score which determines the position in the competition is composed of the mean and the standard deviation of the daily value xt:\n",
    "\n",
    "score =\n",
    "x¯t\n",
    "σ(xt)\n",
    "\n",
    "with score ≡ 0 for σ(xt) = 0.\n",
    "\n",
    "I apply three different algorithms to this problem: logistic regression, neural network and gradient\n",
    "boosting tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Related Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There have been multiple attempts looking into the popular topic of forecasting stock price with\n",
    "techniques of Machine Learning. Based on the works we find, the focus of these research projects\n",
    "vary mainly in three ways.\n",
    "\n",
    "(1) The text information used in prediction ranges from public news, economy trend to exclusive\n",
    "information about the characteristics of the company.\n",
    "\n",
    "(2) The targeting price change can be near-term (high-frequency, less than a minute), short-term\n",
    "(tomorrow to a few days later), and long-term (months later).\n",
    "\n",
    "(3) The set of stocks can be limited to particular stock validation data because of the large sample size. \n",
    "\n",
    "To make it a classification problem, the prediction labels (market residual return in next 10 days) are converted to binary values with 0 representing negative return and 1 representing positive return.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets and features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description\n",
    "\n",
    "All the data used in the project is provided by Kaggle. Two sources of data are provided, one for\n",
    "market data and one for news data, both spanning from 2007 to the end of 2018. The market data\n",
    "contains various financial market information for 3511 US-listed instruments. It is comprised of more\n",
    "than 4 million samples and 16 features.\n",
    "The “returnsOpenNextMktres10” column indicates the market normalized return for the next 10 days\n",
    "and, thus, serves as the ground truth value for the prediction task. The news data contains information\n",
    "at both article level and asset level. There are more 9 million samples and 35 features. Most of the\n",
    "news features are either numerical or type indicators except the “headline” feature, which contains\n",
    "text. The news data provided is intentionally not normalized.\n",
    "Both data sets can be joined by using either the time stamp, asset code or asset name.\n",
    "\n",
    "#### Processing\n",
    "\n",
    "As shown in Figure 1, the stock crashed in late 2008 due to the financial crisis. Thus the stock behaves\n",
    "differently before 2009. Since in the coming 1 year the stock is unlikely to crash, only data after 2009\n",
    "is considered.\n",
    "\n",
    "Figure 1: Closing prices by quantiles\n",
    "    \n",
    "    \n",
    "A large number of samples have null values for features related to normalized market returns, they\n",
    "are filled with the corresponding raw market returns. All features from the market dataset is selected\n",
    "as input. The news dataset, however, are filtered based on feature correlations; highly correlated\n",
    "news features are removed from the training set. For example, sentenceCount and wordCount are\n",
    "highly correlated, so wordCount is removed and sentenceCount is kept. Moreover, outliers with\n",
    "extreme values are removed from market dataset. For example, if the open to close ratio for a single\n",
    "stock is greater than 2, the sample is discarded as an outlier. For the news dataset, most numerical\n",
    "features are clipped between 98 and 2 percentile. The dataset is split into 95% training data and 5%\n",
    "validation data because of the large sample size. To make it a classification problem, the prediction\n",
    "labels (market residual return in next 10 days) are converted to binary values with 0 representing\n",
    "negative return and 1 representing positive return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods Used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression\n",
    "\n",
    "I chose logistic regression as a starting point for establishing a baseline score. The logistic\n",
    "regression takes in all the features as is, such that it does not include higher degree terms. Because of\n",
    "the large size of the training data, small regularization is used.\n",
    "\n",
    "#### Neural network\n",
    "\n",
    "I have fitted a simple one-layer neural network using sigmoid function as the activation function. I have used Keras library. //Here will add more details after updating the model.\n",
    "\n",
    "#### Gradient boosting\n",
    "\n",
    "Gradient boosting is a technique that combines weak predicting models into an ensemble to produce\n",
    "a much stronger one. It is typically implemented on decision trees. Like other boosting algorithms,\n",
    "Gradient boosting is an iterative operation. At each iteration, the algorithm creates a new estimator\n",
    "that minimizes the loss with respect to the current model. This minimization can be approximated by\n",
    "fitting the new estimator to the gradient of loss. The regularization is achieved through several ways: by slowly decreasing the learning rate, setting the number of minimum samples in a tree leaf, limiting number of leaves, or penalizing the complexity of the tree model such as L2 regularization.\n",
    "\n",
    "LightGBM library is used to implement this algorithm in this project. It converts continuous features\n",
    "into bins which reduces memory and boosts speed and grows each tree with the priority given to the\n",
    "leaf with maximum delta loss, leading to lower overall loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Table 1: Validation accuracy, score and test score of the 3 models\n",
    "Metric LR  Neural Network Light-GBM\n",
    "Train-accuracy 0.503 0.561 0.554\n",
    "Val-accuracy 0.485 0.557 0.538\n",
    "Val-score 0.247 0.781 0.731\n",
    "Competition score 0.259 0.645 0.644\n",
    "\n",
    "Table 1 lists the result of the 3 models for the validation dataset and test dataset. The Fully-Connected\n",
    "\n",
    "Neural Network performs the best. And the Light-BGM performs almost as good as the Neural Network. The poor result of the logistic regression is expected because the\n",
    "algorithm assumes linear relationships. Adding news data does not have noticeable effect on the\n",
    "performance. This can be explained by that the news data is more subjective and there are many noises\n",
    "and non-related features. The validation accuracy is similar for all three algorithms. The competition\n",
    "scores, however, are quite different. One explanation could be that the score calculation considers not\n",
    "only the binary prediction, but also market return and standard deviation of the prediction.\n",
    "\n",
    "#### a. AUC curves\n",
    "\n",
    "The AUC score for the Logistic Regression, Neural Network Model and the\n",
    "Light-GBM Model is 0.5, XXX0.5799XXX and 0.5753 respectively, as shown by Figure 3, Figure 4 and\n",
    "Figure 5. The logistic regression behaves similar to a random guess, while the other 2 algorithms\n",
    "show slightly higher ability to predict the market returns.\n",
    "\n",
    "Figure 3: ROC curve for Logistic Regression \n",
    "Figure 4: ROC curve for the FC Neural Network\n",
    "Figure 5: ROC curve for the LGBM\n",
    "\n",
    "#### b. Confusion matrics\n",
    "\n",
    "Table 2: Confusion matrix for LR\n",
    "Pred Class 0 Pred Class 1\n",
    "Class 0 0 1\n",
    "Class 1 0 1\n",
    "\n",
    "Table 3: Confusion matrix for FCNN\n",
    "Pred Class 0 Pred Class 1\n",
    "Class 0 0.458 0.542\n",
    "Class 1 0.347 0.653\n",
    "\n",
    "Table 4: Confusion matrix for LGBM\n",
    "Pred Class 0 Pred Class 1\n",
    "Class 0 0.495 0.504\n",
    "Class 1 0.389 0.611\n",
    "\n",
    "#### c. Output analysis\n",
    "\n",
    "The first model, the Logistic Regression model fails because the model assumes the features and\n",
    "the result are linearly correlated, which is obviously too much simplify the situation. And as\n",
    "expected, the model gets not so good performance.\n",
    "To analyse on the output performance on the three models, as the table 1 showing the training\n",
    "accuracy and the validation accuracy of the three models, they are not overfitting. From the table,\n",
    "we see that the accuracies on training set and validation set are close to each other, which means\n",
    "the models perform similarly on the training set and the validation set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the three attempted algorithms, the neural network performs the best followed closely by\n",
    "the gradient boosting tree, while the logistic regression behaves almost like a random guess. As\n",
    "the logistic regression is fairly a simple algorithm with linear mappings to each feature dimension,\n",
    "its incapability to capture the complex relationship is expected. On the other hand, both the neural\n",
    "network and gradient boosting tree are powerful non-linear algorithms with a large degree of flexibility\n",
    "and control, making them competent to model complex situations.\n",
    "For future work, deeper dive into the feature engineering is needed. It is also worth exploring to\n",
    "combine neural network and gradient boosting tree in an ensemble fashion to produce a stronger\n",
    "model. One of the news features is text based, thus natural language processing can be implemented\n",
    "to extract useful information from it. Given the large parameter sets for the neural network and\n",
    "the gradient boosting tree, achieve the optimum parameters is both difficult and time consuming.\n",
    "However, there is still possible room to make improvement by further tuning the parameters. Lastly,\n",
    "choosing a more powerful baseline such as the support vector machine instead of the simple logistic\n",
    "regression should be considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] XingYu Fu, JinHong Du, YiFeng Guo, MingWen Liu, Tao Dong XiuWen Duan. (2018) A\n",
    "Machine Learning Framework for Stock Selection. arXiv: 1806.1743\n",
    "\n",
    "[2] Yauheniya Shynkevich, TM McGinnity, Sonya Coleman, and Ammar Belatreche. Stock price\n",
    "prediction based on stock-specific and subindustry-specific news articles. In 2015 International\n",
    "Joint Conference on Neural Networks (IJCNN), pages 1–8. IEEE, 2015.\n",
    "\n",
    "[3] Robert P Schumaker and Hsinchun Chen. Textual analysis of stock market prediction using\n",
    "breaking financial news: The azfin text system. ACM Transactions on Information Systems\n",
    "(TOIS), 27(2):12, 2009.\n",
    "\n",
    "[4] Salim Lahmiri. Wavelet low- and high-frequency components as features for predicting stock\n",
    "prices with backpropagation neural networks. Journal of King Saud University, 2014.\n",
    "\n",
    "[5] Milosevic, Nikola. Equity forecast: Predicting long term stock price movement using machine\n",
    "learning. arXiv preprint arXiv:1603.00751 (2016).\n",
    "\n",
    "[6] Nazish Nazir1 , Mudasirahma Dmutto2. Stock Market Prediction Using Artificial Neural Network.\n",
    "International Journal of Advanced Engineering, Management and Science (IJAEMS) Infogain\n",
    "Publication. 2016.\n",
    "\n",
    "[7] Michael Hagenau, Michael Liebmann, and Dirk Neumann. Automated news reading: Stock price\n",
    "prediction based on financial news using context-capturing features. Decision Support Systems,\n",
    "55(3):685–697, 2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some sense we had to create a very simplified version of a trading robot which imposed some requirements on both data science and coding skills in order to make it all work fast. Kernel-only environment set up was one of the most difficult part of the competition since they were rather unstable and inconvenient.\n",
    "\n",
    "My approach:\n",
    "\n",
    "As to model, I had a strong belief that it does not matter which model to use, since signal to noise ratio is low - they will all pick similar 'correlations' from the data if overfitting is properly avoided. I have chosen XGB, tried ET and LGBM. XGB gave the most stable performance. Interestingly, for the majority of years ET outperformed XGB by a slight margin on average, but since there were a couple of years where ET performed much worse ( by 0.15 ), I decided not to follow up with it.\n",
    "\n",
    "As to CV, initial idea was to create an \"approach to trading\", which will be trained on a couple of years data and would be used for next year trading. If some new feature or idea improved the performance of the \"approach to trading\" for most of the years, then it was accepted. However, near the end of competition I noticed that this CV is incorrect as in the data there are periods with stronger signals than in other periods and models trained on those periods outperform other models for most of years even if the time gap is 3-4 years. Thus, CV was yearly but the training was not necessarily on a recent data.\n",
    "\n",
    "Goal of the competition was high Sharpe ratio, making all the standard ML metrics of quality slightly useless. I came up with two approaches to maximize Sharpe ratio. First, given the predictions of a not very precise ML classifier somehow punish volatility in the strategy. I believe that it is a huge luck that there is no payment for the number of assets in the strategy (it seems like on real markets such strategies will suffer from large commissions) and if one uses more assets then the daily return would be more or less stable. On the other hand, it was very important to reduce volatility of the strategy, since, as my portfolio consists of a large number of assets, it will have a lot of market associated risk factors and a lot of correlated assets. I came up with several methods to reduce volatility:\n",
    "\n",
    "Short market portfolio\n",
    "Magic features\n",
    "Normalise predicted confidences by asset volatility\n",
    "Normalise predicted confidences by predictions of quantile models\n",
    "Do some classical portfolio optimization techniques from finance by estimating covariance matrix and search for weights which will maximize Sharpe ratio of a portfolio. This gave a decent boost locally, but I was unable to implement it in kernels given time constraints.\n",
    "Overall, some of those methods worked sometimes, some did not.\n",
    "\n",
    "Second approach was to create an \"IF like\" trading strategy and boost its performance with ML. Sadly, this did not work for me.\n",
    "\n",
    "I did not use news data at all due to running time constraints, but smart usage of topical analysis of news could give you a 0.02 boost on average.\n",
    "\n",
    "It was tough to choose final submissions, as for some years it was optimal to reduce the set of features to only a few, while for other years it was best to preserve a large number of features. The impact of small changes could be really huge - such as 0.1 or 0.2 for some years. For instance, I checked the data from env07 and for this time period models with a very small number of features and harsh restrictions on the information the model obtained about past returns could produce you 0.9+ Sharpe ratio strategies easily.\n",
    "\n",
    "I selected two sets of features and two normalization approaches which worked best on the local CV but performed poorly for 2017 data.\n",
    "\n",
    "\n",
    "**********************\n",
    "\n",
    "I believe that organizers goal of for this competition was not to create a trading model rather than a selection model. (Lyle, Yohn 2018) show that portfolio optimization greatly improves results once outperforming assets are selected, thus your goal would be to predict which company will outperform and then select/optimize them in a portfolio, reducing number of holdings, trading fees and enhancing performance over benchmark. Also they wanted to test the news factor influence on out-performance. More info on this selection+optimization here: https://www.quoniam.com/fileadmin/templates/images/insights/Issue_7_201612_Quoniam_Insights_Where-does-alpha-come-from_EN.pdf\n",
    "\n",
    "**********************\n",
    "\n",
    "\n",
    "Thanks for sharing! It would be great to see your kernel :)\n",
    "\n",
    "I had a couple questions:\n",
    "\n",
    "what are \"magic features\"?\n",
    "How did you use classical sharpe ratio maximizing finance theory? I ask because this competition imposed constraints that are different than the usual portfolio optimization problems in finance. In finance, the main constraint is that the weight of your portfolio <= 1, so one asset could be a much larger proportion of the portfolio. In this competition, you can have a portfolio with weights up with an absolute sum up to n assets, but no one asset can have absolute weight more than 1.\n",
    "\n",
    "\n",
    "ANS1:\n",
    "For 2, you can still do that in this competition. Just divide your weights by a large number like 1e16. It makes all your numbers under 1 and makes sharpe ratio stay the same.\n",
    "\n",
    "ANS2:\n",
    "For 2 Trainer Brock explained the gist of the idea perfectly. Even in this competition format you can do some normalization to make the daily optimization equivalent to the optimization from finance theory.\n",
    "\n",
    "For 1 Those are mostly some operations with asset beta and also some features based on deviations from asset group means\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
